---
title: " FML Assignment 5"
author: "Uma Maheshwari C"
date: "2024-04-07"
output:
  html_document: default
  word_document: default
  pdf_document: default
---



```{r}
#Displaying the required libraries
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
```


Purpose:

The purpose of this assignment is to use Hierarchical Clustering

Directions:

The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for
77 breakfast cereals.


```{r}
library(readr)
Cereals <- read_csv("C:/Users/ujwal/Downloads/Cereals.csv")
View(Cereals)
Num_data <- data.frame(Cereals[,4:16])
```

# Data Preprocessing. Removing all missing values from the dataset.
```{r}
#Missing values are omitted
Num_data <- na.omit(Num_data)
```

```{r}
#using scale function to Normalize the data
Cereals_normalized <- scale(Num_data) #Data is normalized using scale function
```
After pre-processing and scaling, the total number of observations was 74 instead of 77. Only three records had the value "NA."

Question 1 : Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward. Choose the best method.

```{r}
#Using the normalized data to do hierarchical clustering using the Euclidean Distance method.
Dist <- dist(Cereals_normalized, method = "euclidean")
# Hierarchical clustering using Complete Linkage
H_clust <- hclust(Dist, method = "complete")
#the dendogram plotting process.
plot(H_clust, cex = 0.7, hang = -1) #Plots the obtained dendogram
#The dendogram helps us in determining the number of clusters required to classify this dataset.
```

```{r}
#Compute with AGNES and with different linkage methods
single_Hc <- agnes(Cereals_normalized, method = "single")
complete_Hc <- agnes(Cereals_normalized, method = "complete")
average_Hc <- agnes(Cereals_normalized, method = "average")
ward_Hc <- agnes(Cereals_normalized, method = "ward")
```

```{r}
#Choosing the most efficient course of action
print(single_Hc$ac)
print(complete_Hc$ac)
print(average_Hc$ac)
print(ward_Hc$ac)
```
The value approaches 1.0 as the clustering structure becomes more similar. Consequently, the approach with the value closest to 1.0 will be picked. Linkage only: 0.61. Total Connection: 0.84. Linkage on average: 0.78.
Ward Approach: 0.90. According to the results, the Ward technique is the most successful clustering model.

Question 2: How many clusters would you choose?

```{r}
#Using the Ward linkage, 5 clusters seem to be enough for grouping the data.
pltree(ward_Hc, cex = 0.5, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(ward_Hc, k = 5, border = 2:7)
R_Grp <- cutree(ward_Hc, k=5)
Dframe_2 <- as.data.frame(cbind(Cereals_normalized,R_Grp))
```

```{r}
fviz_cluster(list(data = Dframe_2, cluster = R_Grp))
```
# Five clusters can be chosen based on the aforementioned observation.

Question 3 :Comment on the structure of the clusters and on their stability. Hint: To check stability, partition the data and see how well clusters formed based on one part apply to the other
part. To do this:
● Cluster partition A
● Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid).
● Assess how consistent the cluster assignments are compared to the assignments based on all the data. 
```{r}
#Building Partitions: Split_one and split_two
set.seed(123)
split_one <- Num_data[1:55,]
split_two <- Num_data[56:74,]
```

```{r}
#Using Hierarchical Clustering with k = 5 in mind. Compute for the training dataset using AGNES and various linking techniques.
single_CH <- agnes(scale(split_one), method = "single")
complete_CH <- agnes(scale(split_one), method = "complete")
average_CH <- agnes(scale(split_one), method = "average")
ward_CH <- agnes(scale(split_one), method = "ward")
cbind(single=single_CH$ac , complete=complete_CH$ac , average= average_CH $ac , ward= ward_CH$ac)
pltree(ward_CH, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(ward_CH, k = 5, border = 2:7)
cut_2 <- cutree(ward_CH, k = 5)
```

```{r}
# Finding the centroids
HC_result <- as.data.frame(cbind(split_one, cut_2))
HC_result[HC_result$cut_2==1,]
center_1 <- colMeans(HC_result[HC_result$cut_2==1,])
HC_result[HC_result$cut_2==2,]
center_2 <- colMeans(HC_result[HC_result$cut_2==2,])
HC_result[HC_result$cut_2==3,]
center_3 <- colMeans(HC_result[HC_result$cut_2==3,])
HC_result[HC_result$cut_2==4,]
center_4 <- colMeans(HC_result[HC_result$cut_2==4,])
centroid <- rbind(center_1, center_2, center_3, center_4)
x2 <- as.data.frame(rbind(centroid[,-14], split_two))
```

```{r}
Distance_one <- get_dist(x2)
Matrx_one <- as.matrix(Distance_one)
df1 <- data.frame(data=seq(1,nrow(split_two),1), Clusters = rep(0,nrow(split_two)))
for(i in 1:nrow(split_two)) 
  {df1[i,2] <- which.min(Matrx_one[i+4, 1:4])}
df1
cbind(Dframe_2$R_Grp[56:74], df1$Clusters)
table(Dframe_2$R_Grp[56:74] == df1$Clusters)
```
Based on the aforementioned observation, we obtain 7 False and 12 True. As a result, we may say that the model is only partially stable.

Question 4: "The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?”

```{r}
Healthy_HC_Cereals <- Cereals
Healthy_HC_Cereals_RD <- na.omit(Healthy_HC_Cereals)
clust <- cbind(Healthy_HC_Cereals_RD, R_Grp)
clust[clust$R_Grp==1,]
clust[clust$R_Grp==2,]
clust[clust$R_Grp==3,]
clust[clust$R_Grp==4,]
```

```{r}
#Mean ratings are used to select the best cluster.
mean(clust[clust$R_Grp==1,"rating"])
mean(clust[clust$R_Grp==2,"rating"])
mean(clust[clust$R_Grp==3,"rating"])
mean(clust[clust$R_Grp==4,"rating"])
```
Given that Cluster 1 has the greatest value, it might be selected using the previously provided statistics.As a result, Group 1 might be regarded as the cluster associated with a nutritious diet.